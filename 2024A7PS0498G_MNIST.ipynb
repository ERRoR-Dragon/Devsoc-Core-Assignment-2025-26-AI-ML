{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a-nUb4v9gJ-D"
      },
      "outputs": [],
      "source": [
        "# Importing libs\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "DsZRl7HDtZUf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reading data\n",
        "data_train = pd.read_csv('/content/drive/MyDrive/MNIST/train.csv')\n",
        "data_test = pd.read_csv('/content/drive/MyDrive/MNIST/test.csv')\n",
        "\n",
        "data_test.head()"
      ],
      "metadata": {
        "id": "UZVjclAAg5yi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Loading data into an array\n",
        "data_train = np.array(data_train)\n",
        "data_test = np.array(data_test)\n",
        "\n",
        "m, n = data_train.shape\n",
        "a,b = data_test.shape\n",
        "\n",
        "#Shuffling the data to create randomness\n",
        "np.random.shuffle(data_train)\n",
        "np.random.shuffle(data_test)\n",
        "\n",
        "print(m,n,a,b)"
      ],
      "metadata": {
        "id": "gV__Iyvku4rM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_data = data_train[0:m, :]\n",
        "validation_data = data_test[0:a, :]\n",
        "\n",
        "X_train = training_data[:, 1:].T\n",
        "X_train = X_train / 255.0\n",
        "Y_train = training_data[:, 0]\n",
        "\n",
        "X_val = validation_data[:, :].T\n",
        "X_val = X_val / 255.0\n",
        "Y_val = validation_data[:, 0]\n",
        "\n",
        "print(X_val.shape)\n",
        "print(Y_val.shape)\n",
        "print(X_train.shape)\n",
        "print(Y_train.shape)"
      ],
      "metadata": {
        "id": "tn4Mj3Aevcl6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_parameters(): #Putting random numbers to initialize the parameters for the first iteration\n",
        "  W1 = np.random.rand(10, 784) - 0.5\n",
        "  B1 = np.random.rand(10, 1) - 0.5\n",
        "  W2 = np.random.rand(10, 10) - 0.5\n",
        "  B2 = np.random.rand(10, 1) - 0.5\n",
        "  return W1, B1, W2, B2\n",
        "\n",
        "def ReLU(X): # First activation function\n",
        "  return np.maximum(X, 0)\n",
        "\n",
        "def softmax(Z): # Second activation function\n",
        "  return np.exp(Z) / sum(np.exp(Z))\n",
        "\n",
        "def forward_propagation(W1, B1, W2, B2, X): # Forward prop\n",
        "  Z1 = W1.dot(X) + B1\n",
        "  A1 = ReLU(Z1)\n",
        "  Z2 = W2.dot(A1) + B2\n",
        "  A2 = softmax(Z2)\n",
        "  return Z1, A1, Z2, A2\n",
        "\n",
        "def one_hot_converter(Y): # One hot vectorization\n",
        "  one_hot_Y = np.zeros((Y.size, Y.max() + 1))\n",
        "  one_hot_Y[np.arange(Y.size), Y] = 1\n",
        "  return one_hot_Y.T\n",
        "\n",
        "def backward_propagation(W1, B1, W2, B2, Z1, A1, Z2, A2, X, Y): # Backward prop\n",
        "  one_hot_Y = one_hot_converter(Y)\n",
        "  dZ2 = A2 - one_hot_Y\n",
        "  dW2 = 1 / m * dZ2.dot(A1.T)\n",
        "  dB2 = 1 / m * np.sum(dZ2)\n",
        "  dZ1 = W2.T.dot(dZ2) * (Z1 > 0)\n",
        "  dW1 = 1 / m * dZ1.dot(X.T)\n",
        "  dB1 = 1 / m * np.sum(dZ1)\n",
        "  return dW1, dB1, dW2, dB2\n",
        "\n",
        "def update_parameters(W1, B1, W2, B2, dW1, dB1, dW2, dB2, learning_rate):\n",
        "  W1 = W1 - learning_rate * dW1\n",
        "  B1 = B1 - learning_rate * dB1\n",
        "  W2 = W2 - learning_rate * dW2\n",
        "  B2 = B2 - learning_rate * dB2\n",
        "  return W1, B1, W2, B2\n",
        "\n",
        "def cross_entropy_loss(A2, Y): # Our loss function\n",
        "  m = Y.shape[0]\n",
        "  one_hot_Y = one_hot_converter(Y)\n",
        "  # Avoiding log(0) by adding a small epsilon\n",
        "  epsilon = 1e-10\n",
        "  loss = -np.mean(np.sum(one_hot_Y * np.log(A2 + epsilon), axis=0))\n",
        "  return loss\n",
        "\n",
        "def get_predictions(A2):\n",
        "  return np.argmax(A2, 0)\n",
        "\n",
        "def get_accuracy(predictions, Y): # Checks if predicted output is correct and adds them to the counter\n",
        "  return np.sum(predictions == Y) / Y.size\n",
        "\n",
        "def gradient_descent(X, Y, alpha, iterations): # Gradient descent\n",
        "  W1, B1, W2, B2 = initialize_parameters()\n",
        "\n",
        "  iteration_list = []\n",
        "  accuracy_list = []\n",
        "  loss_list = []\n",
        "\n",
        "\n",
        "  for i in range(1,iterations):\n",
        "    Z1, A1, Z2, A2 = forward_propagation(W1, B1, W2, B2, X)\n",
        "    dW1, dB1, dW2, dB2 = backward_propagation(W1, B1, W2, B2, Z1, A1, Z2, A2, X, Y)\n",
        "    W1, B1, W2, B2 = update_parameters(W1, B1, W2, B2, dW1, dB1, dW2, dB2, alpha)\n",
        "\n",
        "    loss = cross_entropy_loss(A2, Y)\n",
        "\n",
        "    if (i%75)==0:\n",
        "\n",
        "      iteration_list.append(i)\n",
        "      loss_list.append(loss) # Updating arrays that is used for plotting\n",
        "      accuracy_list.append(get_accuracy(get_predictions(A2), Y))\n",
        "\n",
        "      print(\"Iteration number: \", i)\n",
        "      print(\"Loss: \", loss)\n",
        "      print(\"Accuracy = \", get_accuracy(get_predictions(A2), Y))\n",
        "      print('\\n')\n",
        "\n",
        "\n",
        "  return W1, B1, W2, B2 , iteration_list, loss_list, accuracy_list\n",
        "\n"
      ],
      "metadata": {
        "id": "O3LiFxWFwc1v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  # Running our model\n",
        "  W1, B1, W2, B2, iterations_for_plot, losses_for_plot, accuracies_for_plot = gradient_descent(X_train, Y_train, 0.1, 750)\n",
        "\n",
        "  # learning rate = 0.1\n",
        "  # iterations = 750"
      ],
      "metadata": {
        "id": "77YlAWOxqFBF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Plot Accuracy vs Iterations\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(iterations_for_plot, accuracies_for_plot)\n",
        "plt.title('Accuracy vs Iterations')\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xticks(np.arange(75, 751, 75))\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Plot Loss vs Iterations\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(iterations_for_plot, losses_for_plot)\n",
        "plt.title('Loss vs Iterations')\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylabel('Loss')\n",
        "plt.xticks(np.arange(75, 751, 75))\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "nQH1IN7JSMJF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}